{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Convolution ninja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a basic convnet model taking as input a tensor of size B x 3 x 224 x 224\n",
    "# and containing the following layers:\n",
    "# - a 2D convolution with output dim = 64, kernel size = 7, stride = 2, padding = 3\n",
    "# - a batch norm\n",
    "# - a ReLU\n",
    "# - a 2D max pooling with kernel size = 3, stride = 2 and padding = 1\n",
    "# - a 2D avg pooling with input dim you should guess using .shape and stride = 1\n",
    "# - a flatten layer\n",
    "# - a linear layer with input dim you should guess using .shape and output dim = 10\n",
    "#\n",
    "# Leave all bias options unchanged (i.e. true by default)\n",
    "#\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.mp = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.ap =  nn.AvgPool2d(56, stride=1)\n",
    "        self.fl = nn.Flatten()\n",
    "        self.ln = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.mp(x)\n",
    "        x = self.ap(x)\n",
    "        x = self.fl(x)\n",
    "        x = self.ln(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123) # do not remove this line\n",
    "X = torch.rand((3, 3, 224, 224))\n",
    "torch.manual_seed(123) # do not remove this line\n",
    "m = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = m(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check. This should return True.\n",
    "torch.allclose(out, torch.tensor([[ 0.23229745,  0.49533808, -0.94218129,  0.80567420, -1.08792305,\n",
    "         -0.08189888,  0.75406575, -1.52529275,  0.54173046, -0.29273897],\n",
    "        [ 0.22363546,  0.49459273, -0.93474990,  0.82280755, -1.07130551,\n",
    "         -0.09333123,  0.75027251, -1.52482963,  0.53764951, -0.28636456],\n",
    "        [ 0.22320433,  0.49914300, -0.93878180,  0.81160194, -1.07520950,\n",
    "         -0.09055193,  0.76426411, -1.52148080,  0.54697251, -0.29279822]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Build a Plain Net\n",
    "Use the notebook to build your architecture. Make sure that inference works before running a training. Do not modify the dataset code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute mean and variance of the CIFAR10 dataset - do not modify this code\n",
    "from torchvision import datasets\n",
    "train_transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_set = datasets.CIFAR10(root='files/', train=True, download=True, transform=train_transform)\n",
    "print(train_set.data.mean(axis=(0,1,2))/255)\n",
    "print(train_set.data.std(axis=(0,1,2))/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this once to load the train and test data straight into a dataloader class\n",
    "# that will provide the batches\n",
    "def data_loader(data_dir,\n",
    "                batch_size,\n",
    "                random_seed=42,\n",
    "                valid_size=0.1,\n",
    "                shuffle=True,\n",
    "                test=False):\n",
    "  \n",
    "    normalize = transforms.Normalize(\n",
    "        mean=[0.4914, 0.4822, 0.4465],\n",
    "        std=[0.2023, 0.1994, 0.2010],\n",
    "    )\n",
    "\n",
    "    # define transforms\n",
    "    transform = transforms.Compose([\n",
    "            transforms.Resize((224,224)),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "    ])\n",
    "\n",
    "    if test:\n",
    "        dataset = datasets.CIFAR10(\n",
    "          root=data_dir, train=False,\n",
    "          download=True, transform=transform,\n",
    "        )\n",
    "\n",
    "        data_loader = torch.utils.data.DataLoader(\n",
    "            dataset, batch_size=batch_size, shuffle=shuffle\n",
    "        )\n",
    "\n",
    "        return data_loader\n",
    "\n",
    "    # load the dataset\n",
    "    train_dataset = datasets.CIFAR10(\n",
    "        root=data_dir, train=True,\n",
    "        download=True, transform=transform,\n",
    "    )\n",
    "\n",
    "    valid_dataset = datasets.CIFAR10(\n",
    "        root=data_dir, train=True,\n",
    "        download=True, transform=transform,\n",
    "    )\n",
    "\n",
    "    num_train = len(train_dataset)\n",
    "    indices = list(range(num_train))\n",
    "    split = int(np.floor(valid_size * num_train))\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.seed(42)\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "    train_idx, valid_idx = indices[split:], indices[:split]\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
    " \n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset, batch_size=batch_size, sampler=valid_sampler)\n",
    "\n",
    "    return (train_loader, valid_loader)\n",
    "\n",
    "\n",
    "# CIFAR10 dataset \n",
    "train_loader, valid_loader = data_loader(data_dir='./files',\n",
    "                                         batch_size=64)\n",
    "\n",
    "test_loader = data_loader(data_dir='./files',\n",
    "                              batch_size=batch_size,\n",
    "                              test=True)\n",
    "                              \n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# functions to show an image\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))\n",
    "images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this cell as a sandbox to dry-run your architecture and make sure you understand the shapes\n",
    "it = iter(train_loader)\n",
    "images, labels = next(it)\n",
    "x = images\n",
    "f1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)\n",
    "print(x.shape, f1(x).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a PlainNet architecture as described in the paper: https://arxiv.org/pdf/1512.03385.pdf\n",
    "#\n",
    "# The module takes as input the number of convolutions at each layer (e.g. [6, 7, 11, 5] for a PlainNet-34)\n",
    "#\n",
    "# Use a for loop for layers 2-5, e.g.:\n",
    "#\n",
    "# layer2 = []\n",
    "# for _ in range(n):\n",
    "#     layer2 += [nn.Conv2d(...), nn.BatchNorm(...), nn.ReLU()]\n",
    "# self.layer2 = nn.Sequential (layer2)\n",
    "#\n",
    "\n",
    "class PlainNet(nn.Module):\n",
    "    def __init__(self, output_dim, layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "        layer2 = []\n",
    "        for _ in range(layers[0]):\n",
    "            layer2 += [\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()]\n",
    "        self.layer2 = nn.Sequential(*layer2)\n",
    "        layer3 = [\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU()]\n",
    "        for _ in range(layers[1]):\n",
    "            layer3 += [\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU()]\n",
    "        self.layer3 = nn.Sequential(*layer3)\n",
    "        layer4 = [\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU()]\n",
    "        for _ in range(layers[2]):\n",
    "            layer4 += [\n",
    "                nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "                nn.BatchNorm2d(256),\n",
    "                nn.ReLU()]\n",
    "        self.layer4 = nn.Sequential(*layer4)\n",
    "        layer5 = [\n",
    "            nn.Conv2d(256,512, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU()]\n",
    "        for _ in range(layers[3]):\n",
    "            layer5 += [\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU()]\n",
    "        self.layer5 = nn.Sequential(*layer5)\n",
    "        self.avgpool = nn.AvgPool2d(7, stride=1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512, output_dim),\n",
    "            nn.Softmax())\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PlainNet(num_classes, layers=[4, 3, 3, 3]).to(device) # plain-18\n",
    "print(f'plain-18: {count_parameters(model)}')\n",
    "# expected answer: 11,529,602"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PlainNet(num_classes, layers=[6, 7, 11, 5]).to(device) # plain-34\n",
    "print(f'plain-34: {count_parameters(model)}')\n",
    "# expected answer: 21,641,474"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that your model can infer correctly\n",
    "# expected answer: torch.Size([64, 10])\n",
    "\n",
    "X = torch.rand((64,3,224,224))\n",
    "model(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "Do not modify the training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "batch_size = 16\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay = 0.001, momentum = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model):\n",
    "\n",
    "    total_step = len(train_loader)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(train_loader):  \n",
    "            # Move tensors to the configured device\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            del images, labels, outputs\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "        print ('Epoch [{}/{}], Loss: {:.4f}' \n",
    "                    .format(epoch+1, num_epochs, loss.item()))\n",
    "                \n",
    "        # Validation\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for images, labels in valid_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                del images, labels, outputs\n",
    "        \n",
    "        print('Accuracy of the network on the {} validation images: {} %'.format(5000, 100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Build a ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a ResidualBlock (i.e regular ResNet block)\n",
    "#\n",
    "# - a 2D convolution with input dim = in_channels, output dim = out_channels, kernel size = 3, stride = stride, padding = 1 and bias = False\n",
    "#   (followed by a batchnorm and a ReLU)\n",
    "# - a 2D convolution with input dim = out_channels, output dim = out_channels, kernel size = 3, stride = 1, padding = 1 and bias = False\n",
    "#   (followed by a batchnorm and a ReLU)\n",
    "#\n",
    "#  x -------> conv1 + BN + ReLU ---------> conv2 + BN -------------- + ----- ReLU ------> out\n",
    "#        |                                                           |\n",
    "#         ---------------------- downsample -------------------------\n",
    "#\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride = 1, downsample = None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "                        nn.Conv2d(in_channels, out_channels, kernel_size = 3, stride = stride, padding = 1, bias = False),\n",
    "                        nn.BatchNorm2d(out_channels),\n",
    "                        nn.ReLU())\n",
    "        self.conv2 = nn.Sequential(\n",
    "                        nn.Conv2d(out_channels, out_channels, kernel_size = 3, stride = 1, padding = 1, bias = False),\n",
    "                        nn.BatchNorm2d(out_channels))\n",
    "        self.downsample = downsample\n",
    "        self.relu = nn.ReLU()\n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        if self.downsample:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "torch.manual_seed(123)\n",
    "X = torch.rand((1,4,3,3))\n",
    "torch.manual_seed(123)\n",
    "m = ResidualBlock(4, 4, stride=1, downsample=None)\n",
    "out = m(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.allclose(out, torch.tensor([[[[1.68696165, 0.04760247, 0.00000000],\n",
    "          [0.59837186, 1.38795805, 1.91975749],\n",
    "          [0.00000000, 0.00000000, 0.00000000]],\n",
    "         [[1.83508623, 0.18320152, 1.34237385],\n",
    "          [0.00000000, 0.00000000, 0.00000000],\n",
    "          [1.56650257, 0.82063377, 1.10089588]],\n",
    "         [[1.69185019, 1.08052707, 1.43224478],\n",
    "          [0.00000000, 0.00000000, 0.70441115],\n",
    "          [1.32906127, 0.19663277, 0.80512488]],\n",
    "         [[0.00000000, 1.80850124, 0.92189598],\n",
    "          [0.00000000, 1.95879388, 0.53320688],\n",
    "          [0.97944093, 0.00000000, 1.19880044]]]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a Bottleneck block\n",
    "#\n",
    "# - a 2D convolution with input dim = in_channels, output dim = out_channels, kernel size = 1, bias = False\n",
    "#   (followed by a batchnorm and a ReLU)\n",
    "# - a 2D convolution with input dim = out_channels, output dim = out_channels, kernel size = 3, stride = stride, padding = 1 and bias = False\n",
    "#   (followed by a batchnorm and a ReLU)\n",
    "# - a 2D convolution with input dim = out_channels, output dim = out_channels * self.expansion, kernel size = 1, bias = False\n",
    "#   (followed by a batchnorm and a ReLU)\n",
    "#\n",
    "#  x -------> conv1 + BN + ReLU --> conv2 + BN + ReLU --> conv3 + BN ------------ + ----- ReLU ------> out\n",
    "#        |                                                                        |\n",
    "#         ------------------------------- downsample -----------------------------\n",
    "#\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "    def __init__(self, in_channels, out_channels, stride = 1, downsample = None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = nn.Sequential(\n",
    "                        nn.Conv2d(in_channels, out_channels, kernel_size = 1, bias = False),\n",
    "                        nn.BatchNorm2d(out_channels),\n",
    "                        nn.ReLU())\n",
    "        self.conv2 = nn.Sequential(\n",
    "                        nn.Conv2d(out_channels, out_channels, kernel_size = 3, stride = stride, padding = 1, bias = False),\n",
    "                        nn.BatchNorm2d(out_channels),\n",
    "                        nn.ReLU())\n",
    "        self.conv3 = nn.Sequential(\n",
    "                        nn.Conv2d(out_channels, out_channels * self.expansion, kernel_size = 1, bias = False),\n",
    "                        nn.BatchNorm2d(out_channels * self.expansion),\n",
    "        )\n",
    "        self.downsample = downsample\n",
    "        self.relu = nn.ReLU()\n",
    "        self.out_channels = out_channels\n",
    "        self.stride = stride\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        out = self.conv3(out)\n",
    "        if self.downsample:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "torch.manual_seed(123)\n",
    "X = torch.rand((1,4,3,3))\n",
    "torch.manual_seed(123)\n",
    "m = Bottleneck(4, 1, stride=1, downsample=None)\n",
    "out = m(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.allclose(out, torch.tensor([[[[0.87262857, 0.87095368, 0.00000000],\n",
    "          [0.00000000, 0.65048909, 0.78769910],\n",
    "          [0.71309650, 0.67899567, 0.76057309]],\n",
    "         [[1.30293083, 0.66962528, 0.16133851],\n",
    "          [0.00000000, 0.77312225, 0.23759353],\n",
    "          [0.97822422, 0.69505239, 1.40387940]],\n",
    "         [[0.95844048, 1.01478648, 0.32792020],\n",
    "          [0.00000000, 1.21308136, 0.90382838],\n",
    "          [0.85085136, 1.23473167, 0.85389799]],\n",
    "         [[1.43384838, 1.25372386, 0.00000000],\n",
    "          [0.00000000, 1.31528068, 0.63905966],\n",
    "          [1.28236103, 1.49217308, 1.01050377]]]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes = 10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.inplanes = 64\n",
    "        self.conv1 = nn.Sequential(\n",
    "                        nn.Conv2d(3, 64, kernel_size = 7, stride = 2, padding = 3, bias = False),\n",
    "                        nn.BatchNorm2d(64),\n",
    "                        nn.ReLU())\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1)\n",
    "        self.layer0 = self._make_layer(block, 64, layers[0], stride = 1)\n",
    "        self.layer1 = self._make_layer(block, 128, layers[1], stride = 2)\n",
    "        self.layer2 = self._make_layer(block, 256, layers[2], stride = 2)\n",
    "        self.layer3 = self._make_layer(block, 512, layers[3], stride = 2)\n",
    "        self.avgpool = nn.AvgPool2d(7, stride=1)\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "        \n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            \n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias = False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer0(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet(ResidualBlock, [2, 2, 2, 2]).to(device) #resnet-18\n",
    "print(count_parameters(model))\n",
    "# expected answer: 11,181,642"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet(ResidualBlock, [3, 4, 6, 3]).to(device) #resnet-34\n",
    "print(count_parameters(model))\n",
    "# expected answer: 21,289,802"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet(Bottleneck, [3, 4, 6, 3]).to(device) #resnet-50\n",
    "print(count_parameters(model))\n",
    "# expected answer: 23,528,522"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check: print the number of parameters for official resnet models\n",
    "model = torchvision.models.resnet50(num_classes=10)\n",
    "print(count_parameters(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: visualize the filters and activation maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a model (saved after training)\n",
    "model.load_state_dict(torch.load('plain-18-sd.bin', map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weights = [] \n",
    "conv_layers = [] \n",
    "model_children = list(model.children())\n",
    "\n",
    "# counter to keep count of the conv layers\n",
    "counter = 0 \n",
    "# append all the conv layers and their respective weights to the list\n",
    "for i in range(len(model_children)):\n",
    "    if type(model_children[i]) == nn.Conv2d:\n",
    "        counter += 1\n",
    "        model_weights.append(model_children[i].weight)\n",
    "        conv_layers.append(model_children[i])\n",
    "    elif type(model_children[i]) == nn.Sequential:\n",
    "        for child in model_children[i]:\n",
    "            if type(child) == nn.Conv2d:\n",
    "                counter += 1\n",
    "                model_weights.append(child.weight)\n",
    "                conv_layers.append(child)\n",
    "print(f\"Total convolutional layers: {counter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the first conv layer filters\n",
    "plt.figure(figsize=(20, 17))\n",
    "print(len(model_weights[1]))\n",
    "for i, filter in enumerate(model_weights[1]):\n",
    "    plt.subplot(4, 5, i+1) # we have 5x5 filters and total of 16 (see printed shapes)\n",
    "    plt.imshow(filter[0, :, :].detach().cpu().numpy(), cmap='viridis')\n",
    "    plt.axis('off')\n",
    "    plt.savefig('filter1.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "it = iter(train_loader)\n",
    "images, labels = next(it)\n",
    "x = images.to(device)\n",
    "data = images\n",
    "print(data.shape)\n",
    "data = torch.unsqueeze(data[0,:,:,:], dim=0)\n",
    "imshow(torchvision.utils.make_grid(data))\n",
    "\n",
    "model(data)\n",
    "\n",
    "k=0\n",
    "act = activation['0'].squeeze()\n",
    "print(act.shape)\n",
    "fig,ax = plt.subplots(8,8,figsize=(12, 15))\n",
    "\n",
    "for i in range(act.size(0)//8):\n",
    "       for j in range(act.size(0)//8):\n",
    "          ax[i,j].imshow(act[k].detach().cpu().numpy())\n",
    "          k+=1    \n",
    "          plt.savefig('fm1.png') \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

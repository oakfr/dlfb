{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to display a network\n",
    "def trace(root):\n",
    "    nodes, edges = set(), set()\n",
    "    def build(v):\n",
    "        if v not in nodes:\n",
    "            nodes.add(v)\n",
    "            for child in v._prev:\n",
    "                edges.add((child, v))\n",
    "                build(child)\n",
    "    build(root)\n",
    "    return nodes, edges\n",
    "\n",
    "def draw_dot(root, format='svg', rankdir='LR'):\n",
    "    \"\"\"\n",
    "    format: png | svg | ...\n",
    "    rankdir: TB (top to bottom graph) | LR (left to right)\n",
    "    \"\"\"\n",
    "    assert rankdir in ['LR', 'TB']\n",
    "    nodes, edges = trace(root)\n",
    "    dot = Digraph(format=format, graph_attr={'rankdir': rankdir}) #, node_attr={'rankdir': 'TB'})\n",
    "    \n",
    "    layer_colors = [\"#FFCCCC\", \"#CCFFCC\", \"#CCCCFF\", \"#FFCC99\", \"#99CCFF\"]\n",
    "    \n",
    "    def get_color(node):\n",
    "        if node._op == '+': return layer_colors[0]\n",
    "        elif node._op == '*': return layer_colors[1]\n",
    "        elif node._op == 'tanh': return layer_colors[2]\n",
    "        # Ajoutez d'autres conditions ici pour d'autres opérations ou couches\n",
    "        return layer_colors[-1]  # Couleur par défaut\n",
    "    \n",
    "    for n in nodes:\n",
    "        color = get_color(n)\n",
    "        dot.node(name=str(id(n)), label=\"{ %s | data %.4f | grad %.4f }\" % (n.label, n.data, n.grad), shape='record', style='filled', fillcolor=color)\n",
    "        if n._op:\n",
    "            dot.node(name=str(id(n)) + n._op, label=n._op)\n",
    "            dot.edge(str(id(n)) + n._op, str(id(n)))\n",
    "    \n",
    "    for n1, n2 in edges:\n",
    "        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n",
    "    \n",
    "    return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "  \n",
    "  def __init__(self, data, _children=(), _op='', label=''):\n",
    "    self.data = data\n",
    "    self.grad = 0.0\n",
    "    self._backward = lambda: None\n",
    "    self._prev = set(_children)\n",
    "    self._op = _op\n",
    "    self.label = label\n",
    "\n",
    "  def __repr__(self):\n",
    "    return f\"Value(data={self.data})\"\n",
    "  \n",
    "  def __add__(self, other): # TODO: ex.1 \n",
    "    other = other if isinstance(other, Value) else Value(other)\n",
    "    out = Value(self.data + other.data, (self, other), '+')\n",
    "    \n",
    "    def _backward(): # ex. 4\n",
    "      self.grad += 1.0 * out.grad\n",
    "      other.grad += 1.0 * out.grad\n",
    "    out._backward = _backward\n",
    "    \n",
    "    return out\n",
    "\n",
    "  def __mul__(self, other): # TODO: ex.1 \n",
    "    other = other if isinstance(other, Value) else Value(other)\n",
    "    out = Value(self.data * other.data, (self, other), '*')\n",
    "    \n",
    "    def _backward(): # ex. 4\n",
    "      self.grad += other.data * out.grad\n",
    "      other.grad += self.data * out.grad\n",
    "    out._backward = _backward\n",
    "      \n",
    "    return out\n",
    "  \n",
    "  def __pow__(self, other): # ex. 6\n",
    "    assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n",
    "    out = Value(self.data**other, (self,), f'**{other}')\n",
    "\n",
    "    def _backward():\n",
    "        self.grad += other * (self.data ** (other - 1)) * out.grad\n",
    "    out._backward = _backward\n",
    "\n",
    "    return out\n",
    "  \n",
    "  def __rmul__(self, other): # other * self\n",
    "    return self * other # TODO: ex.1 \n",
    "\n",
    "  def __truediv__(self, other): # self / other\n",
    "    return self * other**-1 # TODO: ex.1 \n",
    "\n",
    "  def __neg__(self): # -self\n",
    "    return self * -1 # TODO: ex.1 \n",
    "\n",
    "  def __sub__(self, other): # self - other\n",
    "    return self + (-other) # TODO: ex.1 \n",
    "\n",
    "  def __radd__(self, other): # other + self\n",
    "    return self + other # TODO: ex.1 \n",
    "\n",
    "  def tanh(self): # TODO ex.3\n",
    "    x = self.data\n",
    "    t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)\n",
    "    out = Value(t, (self, ), 'tanh')\n",
    "    \n",
    "    def _backward(): # ex. 4\n",
    "      self.grad += (1 - t**2) * out.grad\n",
    "    out._backward = _backward\n",
    "    \n",
    "    return out\n",
    "    \n",
    "  def exp(self): # ex.6\n",
    "    x = self.data\n",
    "    out = Value(math.exp(x), (self, ), 'exp')\n",
    "    \n",
    "    def _backward():\n",
    "      self.grad += out.data * out.grad \n",
    "    out._backward = _backward\n",
    "    \n",
    "    return out\n",
    "    \n",
    "  \n",
    "  def backward(self):\n",
    "    \n",
    "    topo = []\n",
    "    visited = set()\n",
    "    def build_topo(v):\n",
    "      if v not in visited:\n",
    "        visited.add(v)\n",
    "        for child in v._prev:\n",
    "          build_topo(child)\n",
    "        topo.append(v)\n",
    "    build_topo(self)\n",
    "    \n",
    "    self.grad = 1.0\n",
    "    for node in reversed(topo):\n",
    "      node._backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: fill in the Value class to enable the following function. You should get -8.0 as a result.\n",
    "def lol():\n",
    "\n",
    "    a = Value(2.0, label='a')\n",
    "    b = Value(-3.0, label='b')\n",
    "    c = Value(10.0, label='c')\n",
    "    e = a*b; e.label='e'\n",
    "    d = e+c; d.label='d'\n",
    "    f = Value(-2.0, label='f')\n",
    "    L = d*f; L.label='L'\n",
    "    return L\n",
    "\n",
    "print(lol().data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_dot(lol())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: use the lol_grad() function below to manually compute dL/df, dL/dd, dL/de, dL/dc, dL/da, dL/db\n",
    "def lol_grad():\n",
    "    h = 0.0001\n",
    "\n",
    "    a = Value(2.0, label='a')\n",
    "    b = Value(-3.0, label='b')\n",
    "    c = Value(10.0, label='c')\n",
    "    e = a*b; e.label='e'\n",
    "    d = e+c; d.label='d'\n",
    "    f = Value(-2.0, label='f')\n",
    "    L = d*f; L.label='L'\n",
    "    L1 = L.data\n",
    "\n",
    "    a = Value(2.0, label='a')\n",
    "    b = Value(-3.0, label='b')\n",
    "    c = Value(10.0, label='c')\n",
    "    e = a*b; e.label='e'\n",
    "    d = e+c; d.label='d'\n",
    "    d += h\n",
    "    f = Value(-2.0, label='f')\n",
    "    L = d*f; L.label='L'\n",
    "    L2 = L.data\n",
    "\n",
    "    print((L2-L1)/h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# report the values for dL/df, dL/dd, dL/de, dL/dc, dL/da, dL/db in this cell\n",
    "# dL/df = 4.0\n",
    "# dL/dd = 2.0\n",
    "# dL/de = ...\n",
    "# etc.\n",
    "lol_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: implement the function tanh() in the Value class and build the following network\n",
    "# input\n",
    "x1 = Value(2.0, label='x1')\n",
    "x2 = Value(0.0, label='x2')\n",
    "# weights\n",
    "w1 = Value(-3.0, label='w1')\n",
    "w2 = Value(1.0, label='w2')\n",
    "b = Value(6.7, label='b')\n",
    "x1w1 = x1*w1; x1w1.label='x1w1'\n",
    "x2w2 = x2*w2; x2w2.label='x2w2'\n",
    "x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1w1z2w2'\n",
    "n = x1w1x2w2 + b; n.label = 'n'\n",
    "o = n.tanh(); o.label = 'o'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# o.data should be 0.6044\n",
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4: impement the exp() and pow() functions in the Value class. Then implement the backward() function in each Value's operator: __add__, __mul__, __tanh__\n",
    "# inputs x1,x2\n",
    "x1 = Value(2.0, label='x1')\n",
    "x2 = Value(0.0, label='x2')\n",
    "# weights w1,w2\n",
    "w1 = Value(-3.0, label='w1')\n",
    "w2 = Value(1.0, label='w2')\n",
    "# bias of the neuron\n",
    "b = Value(6.8813735870195432, label='b')\n",
    "# x1*w1 + x2*w2 + b\n",
    "x1w1 = x1*w1; x1w1.label = 'x1*w1'\n",
    "x2w2 = x2*w2; x2w2.label = 'x2*w2'\n",
    "x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1*w1 + x2*w2'\n",
    "n = x1w1x2w2 + b; n.label = 'n'\n",
    "# ----\n",
    "#e = (2*n).exp()\n",
    "#o = (e - 1) / (e + 1)\n",
    "o = n.tanh()\n",
    "# ----\n",
    "o.label = 'o'\n",
    "o.backward()\n",
    "draw_dot(o)\n",
    "\n",
    " # expected results:\n",
    " # o.data should be 0.7071\n",
    " # w1.grad = 1.0\n",
    " # w2.grad = 0.0\n",
    " # x1.grad = -1.5\n",
    " # x2.grad = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5: why are we using self.grad += and not self.grad = in the backward() functions?  Answer as a comment in this cell. Hint: create a Value \"a\" and then b = a + a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 6: replace the tanh() function by a combination of exp() and division.  Implement the __exp__ and __pow__ functions in Value. You should get the same results as before.\n",
    "x1 = Value(2.0, label='x1')\n",
    "x2 = Value(0.0, label='x2')\n",
    "# weights w1,w2\n",
    "w1 = Value(-3.0, label='w1')\n",
    "w2 = Value(1.0, label='w2')\n",
    "# bias of the neuron\n",
    "b = Value(6.8813735870195432, label='b')\n",
    "# x1*w1 + x2*w2 + b\n",
    "x1w1 = x1*w1; x1w1.label = 'x1*w1'\n",
    "x2w2 = x2*w2; x2w2.label = 'x2*w2'\n",
    "x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1*w1 + x2*w2'\n",
    "n = x1w1x2w2 + b; n.label = 'n'\n",
    "# ----\n",
    "e = (2*n).exp()\n",
    "o = (e - 1) / (e + 1)\n",
    "#o = ... (no tanh allowed)\n",
    "# ----\n",
    "o.label = 'o'\n",
    "o.backward()\n",
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Congratulations!  You \"unlocked\" the basic layer of pytorch.  Now you know what happens under the hood of the following code. Check that you get the same output as before for grad values.\n",
    "\n",
    "import torch\n",
    "\n",
    "x1 = torch.Tensor([2.0]).double(); x1.requires_grad = True\n",
    "x2 = torch.Tensor([0.0]).double(); x2.requires_grad = True\n",
    "w1 = torch.Tensor([-3.0]).double(); w1.requires_grad = True\n",
    "w2 = torch.Tensor([1.0]).double(); w2.requires_grad = True\n",
    "\n",
    "b = torch.Tensor([6.8813735870195432]).double(); b.requires_grad = True\n",
    "n = x1*w1 + x2*w2 + b\n",
    "o = torch.tanh(n)\n",
    "\n",
    "print(o.data.item())\n",
    "o.backward()\n",
    "\n",
    "print('---')\n",
    "print('x1', x1.grad.item())\n",
    "print('x2', x2.grad.item())\n",
    "print('w1', w1.grad.item())\n",
    "print('w2', w2.grad.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 7: Let's build an MLP based on the Value class.  Fill in the Neuron, Layer and MLP classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "  \n",
    "  def __init__(self, nin):\n",
    "    self.w = [Value(random.uniform(-1,1)) for _ in range(nin)]\n",
    "    self.b = Value(random.uniform(-1,1))\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    # w * x + b\n",
    "    act = sum((wi*xi for wi, xi in zip(self.w, x)), self.b)\n",
    "    out = act.tanh()\n",
    "    return out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return self.w + [self.b]\n",
    "\n",
    "class Layer:\n",
    "  \n",
    "  def __init__(self, nin, nout):\n",
    "    self.neurons = [Neuron(nin) for _ in range(nout)]\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    outs = [n(x) for n in self.neurons]\n",
    "    return outs[0] if len(outs) == 1 else outs\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [p for neuron in self.neurons for p in neuron.parameters()]\n",
    "\n",
    "class MLP:\n",
    "  \n",
    "  def __init__(self, nin, nouts):\n",
    "    sz = [nin] + nouts\n",
    "    self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(nouts))]\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    for layer in self.layers:\n",
    "      x = layer(x)\n",
    "    return x\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [p for layer in self.layers for p in layer.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [2.0, 3.0, -1.0]\n",
    "n = MLP(3, [4, 4, 1])\n",
    "n(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_dot(n(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [\n",
    "  [2.0, 3.0, -1.0],\n",
    "  [3.0, -1.0, 0.5],\n",
    "  [0.5, 1.0, 1.0],\n",
    "  [1.0, 1.0, -1.0],\n",
    "]\n",
    "ys = [1.0, -1.0, -1.0, 1.0] # desired targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(n.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = [n(x) for x in xs]\n",
    "ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = sum([(a-b)**2 for (a,b) in zip(ypred, ys)])\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see how a simple MLP can yield a complex graph already\n",
    "draw_dot(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 8: write a training loop\n",
    "for k in range(20):\n",
    "    # forward pass (compute the loss)\n",
    "    ypred = [n(x) for x in xs]\n",
    "    loss = sum([(a-b)**2 for(a,b) in zip(ypred, ys)])\n",
    "    #backward pass \n",
    "    for p in n.parameters():\n",
    "        p.grad = 0.0\n",
    "    loss.backward()\n",
    "    # update\n",
    "    for p in n.parameters():\n",
    "        p.data += -0.1 * p.grad\n",
    "    print(k, loss.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 9: why do you need to reset the grads to zero at every step?  Answer as a comment in this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 10: try to approximate the n^2 function with a 2-layer MLP.  why is it so hard?\n",
    "# Expected answers:\n",
    "# - our tanh() function requires exp() which quickly goes out of bound\n",
    "# - our compute graph grows quickly with the number of training examples\n",
    "# - needs a lot of data\n",
    "# - needs a more advanced optimizer\n",
    "# - example with keras: https://stackoverflow.com/questions/55170460/neural-network-for-square-x2-approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 100\n",
    "xs = [[a/num] for a in range(-num, num)]\n",
    "ys = [b[0]**2 for b in xs]\n",
    "n = MLP(1,[8,8,1])\n",
    "len(n.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(200):\n",
    "    # forward pass (compute the loss)\n",
    "    ypred = [n(x) for x in xs]\n",
    "    loss = sum([(a-b)**2 for(a,b) in zip(ypred, ys)])\n",
    "    #backward pass \n",
    "    for p in n.parameters():\n",
    "        p.grad = 0.0\n",
    "    loss.backward()\n",
    "    # update\n",
    "    for p in n.parameters():\n",
    "        p.data += -0.001 * p.grad\n",
    "    print(k, loss.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [a/100 for a in range(-200,200)]\n",
    "ys = [n([x]).data for x in xs]\n",
    "yg = [x**2 for x in xs]\n",
    "plt.plot(xs, ys)\n",
    "plt.plot(xs, yg)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
